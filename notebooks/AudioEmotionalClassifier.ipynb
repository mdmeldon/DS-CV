{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/home/mdmeldon/Desktop/Курсовая/DS-CV/venv/lib/python3.8/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning:\n\n\u001b[1mAn import was requested from a module that has moved location.\nImport requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n\n/home/mdmeldon/Desktop/Курсовая/DS-CV/venv/lib/python3.8/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning:\n\n\u001b[1mAn import was requested from a module that has moved location.\nImport of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n\n"
    }
   ],
   "source": [
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.dl.utils import set_global_seed, prepare_cudnn\n",
    "from catalyst.dl.callbacks import AccuracyCallback, AUCCallback, PrecisionRecallF1ScoreCallback, VerboseLogger\n",
    "\n",
    "# Other  \n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "import scipy\n",
    "from scipy.stats import skew\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import sys\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import warnings\n",
    "# ignore warnings \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         labels source                                               path\n0  male_disgust  SAVEE  ../data/raw/surrey-audiovisual-expressed-emoti...\n1  male_disgust  SAVEE  ../data/raw/surrey-audiovisual-expressed-emoti...\n2      male_sad  SAVEE  ../data/raw/surrey-audiovisual-expressed-emoti...\n3  male_neutral  SAVEE  ../data/raw/surrey-audiovisual-expressed-emoti...\n4     male_fear  SAVEE  ../data/raw/surrey-audiovisual-expressed-emoti...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>source</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>male_disgust</td>\n      <td>SAVEE</td>\n      <td>../data/raw/surrey-audiovisual-expressed-emoti...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>male_disgust</td>\n      <td>SAVEE</td>\n      <td>../data/raw/surrey-audiovisual-expressed-emoti...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>male_sad</td>\n      <td>SAVEE</td>\n      <td>../data/raw/surrey-audiovisual-expressed-emoti...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>male_neutral</td>\n      <td>SAVEE</td>\n      <td>../data/raw/surrey-audiovisual-expressed-emoti...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>male_fear</td>\n      <td>SAVEE</td>\n      <td>../data/raw/surrey-audiovisual-expressed-emoti...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "ref = pd.read_csv(\"../data/processed/Data_path.csv\")\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Data Augmentation method   \n",
    "'''\n",
    "def speedNpitch(data):\n",
    "    \"\"\"\n",
    "    Speed and Pitch Tuning.\n",
    "    \"\"\"\n",
    "    # you can change low and high here\n",
    "    length_change = np.random.uniform(low=0.8, high = 1)\n",
    "    speed_fac = 1.2  / length_change # try changing 1.0 to 2.0 ... =D\n",
    "    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n",
    "    minlen = min(data.shape[0], tmp.shape[0])\n",
    "    data *= 0\n",
    "    data[0:minlen] = tmp[0:minlen]\n",
    "    return data\n",
    "\n",
    "'''\n",
    "2. Extracting the MFCC feature as an image (Matrix format).  \n",
    "'''\n",
    "def prepare_data(df, n, aug, mfcc):\n",
    "    X = np.empty(shape=(df.shape[0], n, 216, 1))\n",
    "    input_length = sampling_rate * audio_duration\n",
    "    \n",
    "    cnt = 0\n",
    "    for fname in tqdm(df.path):\n",
    "        file_path = fname\n",
    "        data, _ = librosa.load(file_path, sr=sampling_rate\n",
    "                               ,res_type=\"kaiser_fast\"\n",
    "                               ,duration=2.5\n",
    "                               ,offset=0.5\n",
    "                              )\n",
    "\n",
    "        # Random offset / Padding\n",
    "        if len(data) > input_length:\n",
    "            max_offset = len(data) - input_length\n",
    "            offset = np.random.randint(max_offset)\n",
    "            data = data[offset:(input_length+offset)]\n",
    "        else:\n",
    "            if input_length > len(data):\n",
    "                max_offset = input_length - len(data)\n",
    "                offset = np.random.randint(max_offset)\n",
    "            else:\n",
    "                offset = 0\n",
    "            data = np.pad(data, (offset, int(input_length) - len(data) - offset), \"constant\")\n",
    "\n",
    "        # Augmentation? \n",
    "        if aug == 1:\n",
    "            data = speedNpitch(data)\n",
    "        \n",
    "        # which feature?\n",
    "        if mfcc == 1:\n",
    "            # MFCC extraction \n",
    "            MFCC = librosa.feature.mfcc(data, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "            MFCC = np.expand_dims(MFCC, axis=-1)\n",
    "            X[cnt,] = MFCC\n",
    "            \n",
    "        else:\n",
    "            # Log-melspectogram\n",
    "            melspec = librosa.feature.melspectrogram(data, n_mels = n_melspec)   \n",
    "            logspec = librosa.amplitude_to_db(melspec)\n",
    "            logspec = np.expand_dims(logspec, axis=-1)\n",
    "            X[cnt,] = logspec\n",
    "            \n",
    "        cnt += 1\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_rate=44100\n",
    "# audio_duration=2.5\n",
    "# n_mfcc = 30\n",
    "# mfcc = prepare_data(ref, n = n_mfcc, aug = 0, mfcc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = np.load('../data/processed/MFCC_PREPARE.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(mfcc\n",
    "                                                    , ref.labels\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('../data/processed/MFCC_PREPARE.npy', mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "# y_train = np.eye(len(ref.labels.unique()), dtype='uint8')[lb.fit_transform(y_train)] # to_categorical\n",
    "# y_test = np.eye(len(ref.labels.unique()), dtype='uint8')[lb.fit_transform(y_test)] # to_categorical\n",
    "\n",
    "y_train = lb.fit_transform(y_train) # to_categorical\n",
    "y_test = lb.fit_transform(y_test) # to_categorical\n",
    "\n",
    "# Normalization as per the standard NN process\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=False, progress=True, num_classes=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(X_train)\n",
    "targetsTrain = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "featuresTest = torch.from_numpy(X_test)\n",
    "targetsTest = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "trainSet = TensorDataset(featuresTrain,targetsTrain)\n",
    "validSet = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(trainSet, batch_size = BATCH_SIZE, shuffle = True)\n",
    "valid_loader = DataLoader(validSet, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init catalyst components\n",
    "runner = SupervisedRunner(device=device)\n",
    "loaders = {'train': train_loader, 'valid': valid_loader}\n",
    "logdir = '../data/logs/0'\n",
    "callbacks = [\n",
    "    AccuracyCallback(num_classes = 2),\n",
    "    # AUCCallback(num_classes = 2, class_names = {0: 'Mirni',1: 'Mafia'}),\n",
    "    # VerboseLogger(),\n",
    "    # PrecisionRecallF1ScoreCallback(num_classes = 2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1/20 * Epoch (train): 100% 571/571 [12:06<00:00,  1.27s/it, accuracy01=0.000e+00, loss=2.683]\n1/20 * Epoch (valid): 100% 191/191 [00:41<00:00,  4.60it/s, accuracy01=0.000e+00, loss=1.963]\n[2020-05-28 01:29:36,549] \n1/20 * Epoch 1 (_base): lr=0.0003 | momentum=0.9000\n1/20 * Epoch 1 (train): accuracy01=0.2701 | loss=2.1673\n1/20 * Epoch 1 (valid): accuracy01=0.3312 | loss=2.0148\n2/20 * Epoch (train): 100% 571/571 [12:16<00:00,  1.29s/it, accuracy01=0.000e+00, loss=3.782]\n2/20 * Epoch (valid): 100% 191/191 [00:43<00:00,  4.44it/s, accuracy01=0.000e+00, loss=2.316]\n[2020-05-28 01:42:40,349] \n2/20 * Epoch 2 (_base): lr=0.0003 | momentum=0.9000\n2/20 * Epoch 2 (train): accuracy01=0.4098 | loss=1.6950\n2/20 * Epoch 2 (valid): accuracy01=0.4526 | loss=1.5966\n3/20 * Epoch (train): 100% 571/571 [12:00<00:00,  1.26s/it, accuracy01=0.000e+00, loss=4.302]\n3/20 * Epoch (valid): 100% 191/191 [00:43<00:00,  4.44it/s, accuracy01=1.000, loss=0.244]\n[2020-05-28 01:55:26,435] \n3/20 * Epoch 3 (_base): lr=0.0003 | momentum=0.9000\n3/20 * Epoch 3 (train): accuracy01=0.4649 | loss=1.4958\n3/20 * Epoch 3 (valid): accuracy01=0.4643 | loss=1.9714\n4/20 * Epoch (train): 100% 571/571 [11:59<00:00,  1.26s/it, accuracy01=0.000e+00, loss=3.637]\n4/20 * Epoch (valid): 100% 191/191 [00:43<00:00,  4.42it/s, accuracy01=1.000, loss=1.083]\n[2020-05-28 02:08:10,990] \n4/20 * Epoch 4 (_base): lr=0.0003 | momentum=0.9000\n4/20 * Epoch 4 (train): accuracy01=0.5102 | loss=1.3669\n4/20 * Epoch 4 (valid): accuracy01=0.5314 | loss=1.3419\n5/20 * Epoch (train): 100% 571/571 [12:02<00:00,  1.27s/it, accuracy01=0.000e+00, loss=1.192]\n5/20 * Epoch (valid): 100% 191/191 [00:43<00:00,  4.41it/s, accuracy01=1.000, loss=0.248]\n[2020-05-28 02:20:59,728] \n5/20 * Epoch 5 (_base): lr=0.0003 | momentum=0.9000\n5/20 * Epoch 5 (train): accuracy01=0.5437 | loss=1.2522\n5/20 * Epoch 5 (valid): accuracy01=0.5402 | loss=1.2764\n6/20 * Epoch (train): 100% 571/571 [12:03<00:00,  1.27s/it, accuracy01=0.000e+00, loss=2.107]\n6/20 * Epoch (valid): 100% 191/191 [00:44<00:00,  4.25it/s, accuracy01=1.000, loss=0.607]\n[2020-05-28 02:33:50,975] \n6/20 * Epoch 6 (_base): lr=0.0003 | momentum=0.9000\n6/20 * Epoch 6 (train): accuracy01=0.5818 | loss=1.1652\n6/20 * Epoch 6 (valid): accuracy01=0.5769 | loss=1.1755\n7/20 * Epoch (train): 100% 571/571 [12:01<00:00,  1.26s/it, accuracy01=0.000e+00, loss=2.393]\n7/20 * Epoch (valid): 100% 191/191 [00:43<00:00,  4.38it/s, accuracy01=1.000, loss=0.656]\n[2020-05-28 02:46:39,466] \n7/20 * Epoch 7 (_base): lr=0.0003 | momentum=0.9000\n7/20 * Epoch 7 (train): accuracy01=0.5884 | loss=1.1203\n7/20 * Epoch 7 (valid): accuracy01=0.5661 | loss=1.2214\n8/20 * Epoch (train): 100% 571/571 [12:02<00:00,  1.26s/it, accuracy01=1.000, loss=0.430]\n8/20 * Epoch (valid): 100% 191/191 [00:43<00:00,  4.39it/s, accuracy01=1.000, loss=0.321]\n[2020-05-28 02:59:26,781] \n8/20 * Epoch 8 (_base): lr=0.0003 | momentum=0.9000\n8/20 * Epoch 8 (train): accuracy01=0.6154 | loss=1.0354\n8/20 * Epoch 8 (valid): accuracy01=0.5985 | loss=1.1029\n9/20 * Epoch (train): 100% 571/571 [11:55<00:00,  1.25s/it, accuracy01=0.000e+00, loss=2.936]\n9/20 * Epoch (valid): 100% 191/191 [00:39<00:00,  4.79it/s, accuracy01=1.000, loss=0.837]\n[2020-05-28 03:12:05,310] \n9/20 * Epoch 9 (_base): lr=0.0003 | momentum=0.9000\n9/20 * Epoch 9 (train): accuracy01=0.6428 | loss=0.9830\n9/20 * Epoch 9 (valid): accuracy01=0.5654 | loss=1.2744\n10/20 * Epoch (train): 100% 571/571 [12:01<00:00,  1.26s/it, accuracy01=0.000e+00, loss=4.757]\n10/20 * Epoch (valid): 100% 191/191 [00:44<00:00,  4.29it/s, accuracy01=0.000e+00, loss=2.427]\n[2020-05-28 03:24:53,490] \n10/20 * Epoch 10 (_base): lr=0.0003 | momentum=0.9000\n10/20 * Epoch 10 (train): accuracy01=0.6724 | loss=0.9048\n10/20 * Epoch 10 (valid): accuracy01=0.5825 | loss=1.1229\n11/20 * Epoch (train): 100% 571/571 [12:22<00:00,  1.30s/it, accuracy01=0.000e+00, loss=1.685]\n11/20 * Epoch (valid): 100% 191/191 [00:45<00:00,  4.23it/s, accuracy01=1.000, loss=0.122]\n[2020-05-28 03:38:03,262] \n11/20 * Epoch 11 (_base): lr=0.0003 | momentum=0.9000\n11/20 * Epoch 11 (train): accuracy01=0.6899 | loss=0.8308\n11/20 * Epoch 11 (valid): accuracy01=0.5602 | loss=1.4055\n12/20 * Epoch (train): 100% 571/571 [12:20<00:00,  1.30s/it, accuracy01=0.000e+00, loss=3.784]\n12/20 * Epoch (valid): 100% 191/191 [00:44<00:00,  4.28it/s, accuracy01=0.000e+00, loss=1.287]\n[2020-05-28 03:51:10,669] \n12/20 * Epoch 12 (_base): lr=0.0003 | momentum=0.9000\n12/20 * Epoch 12 (train): accuracy01=0.7194 | loss=0.7740\n12/20 * Epoch 12 (valid): accuracy01=0.5789 | loss=1.5462\n13/20 * Epoch (train):  64% 363/571 [07:43<04:17,  1.24s/it, accuracy01=0.812, loss=0.642]Early exiting\n13/20 * Epoch (train):  64% 363/571 [07:44<04:17,  1.24s/it, accuracy01=0.812, loss=0.642]"
    }
   ],
   "source": [
    "runner.train(\n",
    "        model=model.double(),\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        loaders=loaders,\n",
    "        callbacks=callbacks,\n",
    "        logdir=logdir,\n",
    "        num_epochs=20,\n",
    "        verbose=2,        \n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitvenvvenv6e353e8b5b38496eb3699023619b2036",
   "display_name": "Python 3.8.2 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}